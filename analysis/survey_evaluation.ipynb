{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Combine the Survey datasets\n",
    "\n",
    "There are two batches of ratings as the first was more of a test how MTurk works\n",
    "\n",
    "The image order was scrambled for each image so the order has less of an effect on ratings, therefore it needs to be\n",
    "unscrambled.\n",
    "\n",
    "Furthermore unapproved ratings (like ones containing only default values) need tp be filtered out"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7.0\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import survey_utils as utils\n",
    "import random\n",
    "import math\n",
    "\n",
    "from pathlib import Path\n",
    "from IPython.core.display import display, HTML\n",
    "import scipy\n",
    "\n",
    "print(scipy.__version__)\n",
    "\n",
    "# Enables visualizations\n",
    "visualization = True\n",
    "relevant_columns = utils.get_relevant_columns_for_visualization(visualization)\n",
    "\n",
    "# styles to evaluate further\n",
    "styles = ['original', 'nicer', 'ssmtpiaa_sgd', 'ssmtpiaa_cma', 'expert']\n",
    "\n",
    "# Load the datasets from both batches\n",
    "survey1 = pd.read_csv(Path('sets/survey_results/processed/MTurk_Batch_1_approvals.csv'))\n",
    "survey2 = pd.read_csv(Path('sets/survey_results/processed/MTurk_Batch_2_approvals.csv'))\n",
    "\n",
    "# Combine the 2 suvey datasets and descramble ratings.\n",
    "survey_result = utils.preprocess_data(survey1, survey2, visualization)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How often is a style the preffered style?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "          prefered_in\noriginal         1312\nexpert           3372",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>prefered_in</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>original</th>\n      <td>1312</td>\n    </tr>\n    <tr>\n      <th>expert</th>\n      <td>3372</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "styles_to_evaluate = ['original', 'expert']\n",
    "\n",
    "# initialize counter\n",
    "best_styles = {}\n",
    "examples = {}\n",
    "for style in styles_to_evaluate:\n",
    "    best_styles[style] = 0\n",
    "    examples[style] = set()\n",
    "\n",
    "for i in range(survey_result.shape[0]):\n",
    "    best = 0\n",
    "    # Get best rating for each image\n",
    "    for style in styles_to_evaluate:\n",
    "        best = max(best, survey_result.at[i, f'{style}_rating'])\n",
    "    # Find out to which style it belongs\n",
    "    for style in styles_to_evaluate:\n",
    "         if survey_result.at[i, f'{style}_rating'] == best:\n",
    "             best_styles[style] += 1\n",
    "             examples[style].add(i)\n",
    "\n",
    "best_count = pd.DataFrame(index=styles_to_evaluate)\n",
    "for style in styles_to_evaluate:\n",
    "    best_count.at[style, 'prefered_in'] = best_styles[style]\n",
    "best_count.prefered_in = best_count.prefered_in.astype(int)\n",
    "display(best_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Significance test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "base_style = 'original'\n",
    "enhanced_style = 'expert'\n",
    "\n",
    "enhanced_is_better = []\n",
    "random_list = []\n",
    "\n",
    "for i in range(survey_result.shape[0]):\n",
    "    best = 0\n",
    "    # Get best rating for each image\n",
    "    for style in [base_style, enhanced_style]:\n",
    "        best = max(best, survey_result.at[i, f'{style}_rating'])\n",
    "    # Find out to which style it belongs\n",
    "    for style in [base_style, enhanced_style]:\n",
    "        if survey_result.at[i, f'{base_style}_rating'] == best:\n",
    "            enhanced_is_better.append(0)\n",
    "        else:\n",
    "            enhanced_is_better.append(1)\n",
    "pvalues = []\n",
    "statistics = []\n",
    "\n",
    "for i in range(50):\n",
    "    result = stats.ttest_ind(enhanced_is_better, utils.get_random_array(len(enhanced_is_better)), alternative='greater')\n",
    "    pvalues.append(result.pvalue)\n",
    "    statistics.append(result.statistic)\n",
    "\n",
    "print(f'statistic: {np.mean(statistics)}')\n",
    "print(f'p-value: {np.mean(pvalues)}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "statistic: 30.33327781556948\n",
      "p-value: 1.2653111283065755e-183\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'scipy.stats' has no attribute 'binomtest'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-74-815099b2c086>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[0mp\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0.5\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 20\u001B[1;33m \u001B[0mbinom_test\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mstats\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinomtest\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     21\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     22\u001B[0m \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mstats\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbinom_test\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mk\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mn\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0malternative\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m'greater'\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'scipy.stats' has no attribute 'binomtest'"
     ]
    }
   ],
   "source": [
    "base_style = 'original'\n",
    "enhanced_style = 'ssmtpiaa_sgd'\n",
    "\n",
    "enhanced_is_better = []\n",
    "random_list = []\n",
    "\n",
    "for i in range(survey_result.shape[0]):\n",
    "    best = 0\n",
    "    # Get best rating for each image\n",
    "    for style in [base_style, enhanced_style]:\n",
    "        best = max(best, survey_result.at[i, f'{style}_rating'])\n",
    "    # Find out to which style it belongs\n",
    "    if survey_result.at[i, f'{enhanced_style}_rating'] == best:\n",
    "        enhanced_is_better.append(1)\n",
    "\n",
    "n = survey_result.shape[0]\n",
    "k = len(enhanced_is_better)\n",
    "p = 0.5\n",
    "\n",
    "binom_test = stats.binomtest()\n",
    "\n",
    "print(type(stats.binom_test(k, n, p, alternative='greater')))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Simple means over all ratings"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for style in styles:\n",
    "    print(f\"Mean rating for {style}: {np.mean(survey_result[style]):.2f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Combine individual ratings of an image"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "survey_grouped = utils.combine_image_ratings(survey_result, visualization, styles)\n",
    "\n",
    "# select row to preview (between 0 and 499)\n",
    "row = 20\n",
    "\n",
    "if visualization:\n",
    "    display(HTML(pd.DataFrame(survey_grouped.iloc[row][relevant_columns]).transpose().to_html(escape=False)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What are the mean variances of each style?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ratings = pd.DataFrame(index=styles)\n",
    "\n",
    "for style in styles:\n",
    "    ratings.at[style, 'rating'] = np.mean(survey_result[style])\n",
    "    ratings.at[style, 'variance'] = np.var(survey_result[style])\n",
    "    ratings.at[style, 'standard deviation'] = np.std(survey_result[style])\n",
    "\n",
    "display(ratings)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## How often is a style the preffered style (via mean rating)?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "styles_to_evaluate = ['nicer', 'ssmtpiaa_sgd']\n",
    "\n",
    "# initialize counter\n",
    "best_styles = {}\n",
    "examples = {}\n",
    "for style in styles_to_evaluate:\n",
    "    best_styles[style] = 0\n",
    "    examples[style] = set()\n",
    "\n",
    "for i in range(survey_grouped.shape[0]):\n",
    "    best = 0\n",
    "    # Get best rating for each image\n",
    "    for style in styles_to_evaluate:\n",
    "        best = max(best, survey_grouped.at[i, f'{style}_rating'])\n",
    "    # Find out to which style it belongs\n",
    "    for style in styles_to_evaluate:\n",
    "         if survey_grouped.at[i, f'{style}_rating'] == best:\n",
    "             best_styles[style] += 1\n",
    "             examples[style].add(i)\n",
    "best_count = pd.DataFrame(index=styles_to_evaluate)\n",
    "for style in styles_to_evaluate:\n",
    "    best_count.at[style, 'best_style'] = best_styles[style]\n",
    "best_count.best_style = best_count.best_style.astype(int)\n",
    "display(best_count)\n",
    "\n",
    "if visualization:\n",
    "    for style in styles_to_evaluate:\n",
    "        example = random.choice(list(examples[style]))\n",
    "        diff_to_orig = survey_grouped.iloc[example][f'{style}_rating'] - survey_grouped.iloc[example][\"original_rating\"]\n",
    "        display(f'Example where {style} was the preffered style with a : difference of {diff_to_orig:.2f} to the original rating')\n",
    "        display(HTML(pd.DataFrame(survey_grouped.iloc[example][relevant_columns]).transpose().to_html(escape=False)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}